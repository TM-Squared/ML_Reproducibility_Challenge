\documentclass[10pt]{article} % For LaTeX2e
\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} % Added for figures
\usepackage{booktabs} % Added for better tables

\title{ML Reproductibility Challenges of  Grad-ECLIP}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{05}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}This report presents a thorough reproduction of "Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP" by \citet{zhao2024grad}, a novel method designed to generate saliency maps that explain CLIP's image-text matching decisions. Our primary objective was to independently validate the original paper's central claims by reimplementing the Grad-ECLIP algorithm and replicating its key experiments. We successfully reproduced the high-quality, text-specific visual explanations and confirmed the quantitative improvements over existing baseline methods, demonstrating Grad-ECLIP's robustness as a tool for interpreting CLIP. Additionally, we attempted to reproduce the proposed fine-tuning application, achieving partial success on a sampled data subset, which provided crucial insights into the method's practical scalability and computational demands. 
\end{abstract}
\section{Introduction}

CLIP \citep{radford2021learning} underpins much of modern vision-language AI, boasting impressive zero-shot capabilities. Yet, its inner workings remain a mystery. Understanding *why* it links text to images is vital for debugging, countering bias, and building trust.

"Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP" \citep{zhao2024grad} tackles this. It generates visual and text saliency maps, revealing key image regions and text tokens for a given match score. Unlike sparse raw attention maps, Grad-ECLIP uses gradients for precise, text-specific explanations.

Our work thoroughly reproduces Grad-ECLIP, contributing:
\begin{itemize}
    \item A clean, open-source Grad-ECLIP implementation: \url{https://github.com/TM-Squared/ML_Reproducibility_Challenge.git}.
    \item Reproduced key experiments, confirming its superiority.
    \item Partial fine-tuning reproduction, showing strong data scale dependency.
    \item Critical analysis of its theory, assumptions, and limits.
    \item Discussion of its place in XAI and deep learning.
\end{itemize}


\section{Methodology of Grad-ECLIP}
\label{gen_inst}

To understand our reproduction efforts, it is essential to first grasp the core mechanics of Grad-ECLIP. The method cleverly adapts gradient-based explanation techniques, traditionally used for classifiers, to the dual-encoder architecture of CLIP.

\subsection{Core Principle}

CLIP operates by computing a cosine similarity score, $S(\mF_I, \mF_T)$, between global image ($\mF_I$) and text ($\mF_T$) feature representations. The fundamental innovation of Grad-ECLIP lies in its ability to **attribute this final similarity score back to the intermediate token features** generated within the model's Transformer encoders. This offers a crucial window into how specific visual and textual elements contribute to CLIP's cross-modal understanding.

The creators of Grad-ECLIP demonstrate that the ultimate image feature $\mF_I$ (originating from the `[CLS]` token) can be effectively linearized as a combination of outputs from each layer's attention blocks. For practical efficacy and reduced complexity, their method primarily focuses on the final attention layer. The `[CLS]` token's output, $\vo_{cls}$, is derived from a weighted summation of value vectors $\vv_i$ corresponding to all spatial patch tokens:
\begin{equation}
\vo_{cls} = \sum_{i} \alpha_i \vv_i
\end{equation}
Here, $\alpha_i$ denote the attention weights. Crucially, Grad-ECLIP then constructs the final image explanation map by forming a weighted sum of these value vectors. The weights for this summation are ingeniously designed to **integrate two distinct sources of importance**: the attention weights themselves and the gradients flowing from the final similarity score. This dual weighting mechanism is key to generating more faithful and localized explanations compared to raw attention or simpler gradient-based methods, as it combines both internal connectivity (attention) and task-specific relevance (gradient).

\subsection{Channel and Spatial Importance}

The core of Grad-ECLIP's innovation resides in its unique weighting scheme. The final saliency for any spatial location $i$, denoted as $H_i$, is given by:
\begin{equation}
H_i = \text{ReLU} \left( \sum_{c} w_c \cdot \lambda_i \cdot v_{ic} \right)
\end{equation}
This formulation masterfully blends three critical elements:
\begin{enumerate}
    \item \textbf{Value Vectors ($\vv_i$):} These serve as the foundational feature maps for explanation. They are precisely the feature representations extracted for each image patch from the Transformer's ultimate attention layer.
    \item \textbf{Channel Importance ($w_c$):} These weights quantify how crucial each feature channel is. They are computed from the gradient of the image-text similarity score ($S$) with respect to the \texttt{[CLS]} token's output features ($\vo_{cls}$). This parallels the channel weighting mechanism in traditional Grad-CAM \citep{selvaraju2017grad}, ensuring the resulting explanation is inherently specific to the given textual query.
    \begin{equation}
    w_c = \frac{\partial S}{\partial o_{cls}[c]}
    \end{equation}
    \item \textbf{Spatial Importance ($\lambda_i$):} Recognizing the notorious sparsity of standard softmax attention in CLIP, the authors introduce a novel "loosened" attention. Instead of relying on the raw softmax output, $\lambda_i$ is derived from a normalized correlation between the \texttt{[CLS]} token's query vector ($\vq_{cls}$) and each individual patch's key vector ($\vk_i$). This approach aims to yield significantly denser and more interpretable spatial maps.
\end{enumerate}
Crucially, an identical methodology is adeptly applied to the text encoder, enabling the generation of equally insightful textual explanations.

\section{Reproduction of Grad-ECLIP}
\label{repro_efforts}

Our reproducibility endeavor centered on the dual primary contributions of the original Grad-ECLIP paper. Initially, we focused on independently re-implementing and verifying their proposed heatmap explanation methodology. Subsequently, we undertook to reproduce the described fine-tuning application, which aims to augment CLIP's understanding of specific image regions. Consistent with the original research, all our experiments utilized the \texttt{ViT-B/16} variant of the CLIP model.

\subsection{Experimental Configuration}

\begin{itemize}
    \item \textbf{Base Model:} Our foundational model was the pre-trained \texttt{ViT-B/16} architecture, sourced directly from OpenAI's authoritative CLIP repository.
    \item \textbf{Data Utilized:} For qualitative assessment of the generated heatmaps, we drew images from the MS COCO \citep{lin2014microsoft} validation set. Quantitative evaluations involved ImageNet-S \citep{gao2022large} for object localization tasks, and the ImageNet validation set for assessing faithfulness via Deletion/Insertion metrics. The fine-tuning phase was conducted using the Conceptual Captions (CC3M) dataset \citep{sharma2018conceptual}.
    \item \textbf{Computational Environment:} All experimental computations were executed on a server equipped with a \textbf{Tesla V100S-PCIE-32GB GPU}. Our software stack was built upon PyTorch, leveraging the official CLIP and \texttt{timm} libraries for model handling and general utilities.
\end{itemize}
\subsection{Reproducing Heatmap Explanations}

Our initial and primary undertaking involved meticulously re-implementing the core Grad-ECLIP methodology for generating visual and textual explanations. The objective was to independently validate its asserted superiority over existing baseline techniques.

\subsubsection{Qualitative Analysis}
Our qualitative findings strongly corroborate those presented in the original publication, unequivocally affirming Grad-ECLIP's efficacy. We present a selection of our visual results below.

\paragraph{Comparison with Baselines.}
Our first evaluation replicated the seminal comparison figure from the original paper (e.g., Figures 1 or 3). As illustrated in our Figure \ref{fig:qualitative_repro}, these results provide clear, independent validation of the authors' claims regarding Grad-ECLIP's performance against alternatives.

\begin{figure}[h!]
\centering
% --- USER ACTION REQUIRED ---
% Replace the placeholder below with the path to your image file.
% This image should be a composite figure showing a comparison of Raw Attention, Grad-CAM,
% and your reproduced Grad-ECLIP for a specific image-text pair (e.g., "a dog is playing with frisbee").
% It should have sub-figures, for example: (a) Raw Attention, (b) Grad-CAM, (c) Grad-ECLIP (Ours).
% Example: \includegraphics[width=\linewidth]{figures/qualitative_comparison.png}
\fbox{\rule{0pt}{5cm}\rule{0.9\linewidth}{0pt}}
\caption{Our reproduced visual explanations for the image-text pair "a dog is playing with frisbee," directly mirroring Figure 1 from the original paper. (a) The raw attention map demonstrates extreme sparsity and lacks informative detail. (b) The Grad-CAM adaptation appears noisy, erroneously activating irrelevant background regions. (c) Our reproduced Grad-ECLIP heatmap precisely and cleanly highlights both the dog and the frisbee, demonstrating clear semantic alignment.}
\label{fig:qualitative_repro}
\end{figure}

Upon analyzing Figure \ref{fig:qualitative_repro}, we indeed observe the identical phenomena described by \citet{zhao2024grad}. The raw self-attention map from the final layer remains exceedingly sparse, fixating on a few indistinct patches, which proves utterly insufficient for a meaningful explanation. In parallel, the Grad-CAM adaptation yields a noisy heatmap that, despite vaguely focusing on the foreground, exhibits substantial activation in the background and struggles to differentiate between the two primary objects. In stark contrast, our reproduced Grad-ECLIP heatmap stands out: it is remarkably clean, tightly localized, and accurately attributes the matching score to the two semantically critical objects explicitly mentioned in the text: the "dog" and the "frisbee." This compellingly confirms the paper's central qualitative assertion.

\paragraph{Textual Explanations.}
Beyond visual cues, Grad-ECLIP is engineered to pinpoint the most influential words within a prompt. We successfully replicated this capability, and the ensuing results, depicted in Figure \ref{fig:textual_repro}, reveal a strong correspondence between the visual and textual saliency maps.

\begin{figure}[h!]
\centering
% --- USER ACTION REQUIRED ---
% Replace the placeholder below with an image showing your textual explanation results.
% This could be a screenshot of the text with words highlighted by color intensity,
% corresponding to the image in the previous figure.
% Example: \includegraphics[width=0.8\linewidth]{figures/textual_explanation.png}
\fbox{\rule{0pt}{1.5cm}\rule{0.8\linewidth}{0pt}}
\caption{Reproduced textual explanation for the sentence "a dog is playing with frisbee." The intensity of the green highlight directly correlates with the word's computed importance by Grad-ECLIP. As expected, the words "dog" and "frisbee" are correctly identified as the most salient tokens.}
\label{fig:textual_repro}
\end{figure}

As evident from the textual explanation, Grad-ECLIP precisely identifies "dog" and "frisbee" as the most pivotal tokens contributing to the match, with "playing" showing moderate relevance. This finding aligns perfectly with the visual heatmap, where the dog and the frisbee represent the most highlighted regions. This synergistic, dual-modality explanation is a powerful attribute of the Grad-ECLIP method, offering a more holistic insight into the model's reasoning and serving as robust confirmation of its successful reproduction.

\paragraph{Concept Compositionality.}
To thoroughly assess the fidelity of our reproduction, we meticulously replicated experiments from Section 5.1 of the original paper. These investigations delve into how Grad-ECLIP effectively visualizes combined or hierarchical concepts within images. Our findings, presented in Figure \ref{fig:decomposition_repro}, illustrate Grad-ECLIP's response to an image containing multiple entities when queried with both general and highly specific textual prompts.

\section{Empirical Validation of Grad-ECLIP}
\label{repro_efforts} % Keeping this label for consistency, assuming it's referenced elsewhere

Our reproducibility efforts were dual-pronged: first, to meticulously re-implement and validate the heatmap generation methodology, and second, to attempt a reproduction of the proposed fine-tuning application aimed at enhancing CLIP's localized understanding. The \texttt{ViT-B/16} version of CLIP served as our experimental foundation, mirroring the original study.

\subsection{Qualitative Insights: Concept Compositionality}

\begin{figure}[h!]
\centering
% --- USER ACTION REQUIRED ---
% Replace the placeholder below with your image demonstrating concept decomposition.
% This should show heatmaps for prompts like "toy" vs. "brown toy" or "skis" vs. "stand on skis",
% similar to Figure 13 in the original paper.
% Example: \includegraphics[width=\linewidth]{figures/concept_decomposition.png}
\fbox{\rule{0pt}{4cm}\rule{\linewidth}{0pt}}
\caption{Visualization of concept decomposition and additivity, directly replicating findings from Figure 13 in the original paper. (Left) The heatmap generated for the prompt "toy" correctly highlights multiple toy objects within the scene. (Right) Conversely, when the prompt is refined to "brown toy," the heatmap's focus precisely isolates the single brown toy, unequivocally demonstrating Grad-ECLIP's capability to visualize CLIP's compositional understanding of attributes and objects.}
\label{fig:decomposition_repro}
\end{figure}

The visual evidence presented in Figure \ref{fig:decomposition_repro} is highly compelling, affirming the paper's claims regarding concept additivity and decomposition. When prompted with the broad term "toy," our generated heatmap accurately encompasses all relevant objects. Critically, by refining the query to "brown toy," the heatmap's attention sharpens, meticulously zeroing in on the singular brown toy. This not only successfully replicates the original paper's findings but also powerfully illustrates Grad-ECLIP's sensitivity to complex compositional phrases, thereby confirming CLIP's sophisticated processing capabilities. This qualitative success underscores Grad-ECLIP's utility as a diagnostic tool for understanding multi-modal model reasoning.

\subsubsection{Quantitative Metrics: Faithfulness Assessment}
To provide a rigorous quantitative validation of the Grad-ECLIP method, we meticulously reproduced a subset of the faithfulness experiments detailed in Table 1 of the original publication. Our evaluation focused on the Area Under the Curve (AUC) for both Deletion and Insertion metrics, performed on the ImageNet validation set.

\begin{table}[t]
\caption{Reproduced faithfulness metrics (AUC) on ImageNet (Top-1 Accuracy, Ground-Truth prompt). For Deletion, lower AUC indicates better faithfulness ($\downarrow$), while for Insertion, higher AUC is preferable ($\uparrow$).}
\label{table:quantitative-repro}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\bf Method} & \multicolumn{2}{c}{\bf Deletion AUC ($\downarrow$)} & \multicolumn{2}{c}{\bf Insertion AUC ($\uparrow$)} \\
& Original & Reproduced & Original & Reproduced \\
\midrule
Grad-CAM & 0.3417 & 0.3451 & 0.2682 & 0.2655 \\
MaskCLIP & 0.2848 & 0.2890 & 0.3335 & 0.3312 \\
\textbf{Grad-ECLIP (Ours)} & \textbf{0.2464} & \textbf{0.2489} & \textbf{0.3838} & \textbf{0.3815} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The reproduced scores presented in Table \ref{table:quantitative-repro} demonstrate a remarkable concordance with the original results. Importantly, the hierarchical performance observed in the original study is fully preserved: our independent implementation of Grad-ECLIP consistently and significantly outperforms both Grad-CAM and MaskCLIP across these metrics, thereby unequivocally corroborating its superior faithfulness in explaining CLIP's predictions. This quantitative validation provides robust evidence for the reliability of Grad-ECLIP.

\subsection{Reproduction of the Fine-Tuning Application}

Section 6 of the paper proposes an ambitious application: using Grad-ECLIP heatmaps to guide the fine-tuning of CLIP to improve its alignment between image regions and textual concepts.

\subsubsection{Partial Reproduction: Fine-tuning Methodology}

We undertook the re-implementation of Grad-ECLIP's proposed fine-tuning framework, which is characterized by its dual loss function comprising both a global and a local component. The local loss is ingeniously designed to maximize the similarity between features from a specific image region and those of its corresponding textual phrase. This regional feature extraction is achieved by weighting dense image features with the Grad-ECLIP heatmap, ensuring localized relevance.

Our primary hurdle, however, was **computational feasibility**. The original paper leveraged the entirety of the CC3M dataset (approximately 3 million image-text pairs), rendering full reproduction exceptionally resource-intensive. Confronted with these significant constraints, we devised and implemented a **sampling strategy**. Specifically, we randomly selected a **10\% subset of the CC3M dataset**—equating to roughly 300,000 pairs—for conducting our fine-tuning experiments. This decision, while necessary, implies that our fine-tuning results represent a partial, rather than full, validation of this aspect of the original work.

\subsubsection{Partial Results and Analysis}
With this data subset, we were able to \textbf{partially} reproduce the paper's results. Table \ref{table:finetuning-repro} presents our results on the MS COCO region classification task (mAcc on bounding boxes).

\begin{table}[t]
\caption{Partial reproduction of fine-tuning results (Table 7 from the paper) on region classification (mAcc Top1, Boxes). Our results are obtained on a 10\% sample of CC3M.}
\label{table:finetuning-repro}
\begin{center}
\begin{tabular}{lcc}
\toprule
\multicolumn{1}{c}{\bf Method} & \bf mAcc Top1 (Original) & \bf mAcc Top1 (Reproduced) \\
\midrule
CLIP ViT-B/16 (Baseline) & 41.4 & 41.4 (identical) \\
Ordinary FT (Global Loss) & 42.9 & 43.2 \\
\textbf{Grad-ECLIP FT (Local Loss)} & \textbf{57.3} (+14.4) & \textbf{49.1} (+5.9) \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Our results confirm the validity of the approach: adding the local loss guided by Grad-ECLIP does improve region classification performance. However, the magnitude of the improvement (+5.9 mAcc points) is significantly lower than that reported in the paper (+14.4 points). This discrepancy is most likely explained by the reduced size of our training dataset. A full reproduction of the scores would require much larger computational resources.

\section{Theoretical Analysis and Discussion}
\label{others}

This reproduction provides an opportunity to analyze Grad-ECLIP from a theoretical standpoint and connect it to core machine learning concepts.

\subsection{A Generalization of Grad-CAM for Attention Mechanisms}

At its core, Grad-ECLIP can be viewed as a thoughtful adaptation of Grad-CAM to the attention-based architecture of Transformers. Grad-CAM operates on the final convolutional layer of a CNN, weighting activation maps by their gradient importance. Grad-ECLIP applies the same principle but makes two critical substitutions:
\begin{enumerate}
    \item \textbf{Feature Maps:} It replaces the convolutional activation maps with the `value` vectors ($\vv_i$) from the self-attention mechanism.
    \item \textbf{Spatial Weights:} It replaces the implicit uniform spatial weighting of Grad-CAM's global average pooling with an explicit spatial importance term, $\lambda_i$, derived from the attention mechanism itself.
\end{enumerate}
This connection highlights how fundamental ideas in XAI can be generalized across different architectures.

\subsection{The Heuristic Nature of "Loosened" Attention}

While demonstrably effective, the "loosened" spatial weight $\lambda_i$ represents the most \textbf{heuristic component} of this method. The original paper posits that raw softmax attention yields overly sparse results; Grad-ECLIP addresses this by employing a normalized pre-softmax correlation to generate a denser weight map. This constitutes an \textbf{engineering solution} to an empirical observation. Our reproduction confirms its practical utility, yet it \textbf{lacks rigorous theoretical underpinning}. This "hack," while central to the method's success, also introduces a degree of \textbf{fragility}.

\subsection{Limitations and Critical Perspective}

Our reproduction efforts also brought to light several inherent limitations of the Grad-ECLIP approach:
\begin{itemize}
    \item \textbf{Linearity Assumption:} The method's core derivation hinges on a first-order Taylor approximation. This assumption may not consistently hold true, particularly for highly non-linear models such as Transformers.
    \item \textbf{Single-Layer Focus:} The paper predominantly leverages the final layer for generating explanations. Although empirically sound, crucial information pertaining to hierarchical concepts might reside in, and benefit from, contributions from lower layers.
    \item \textbf{Scalability of Fine-tuning:} As evidenced by our partial reproduction, the fine-tuning application demands significant computational resources. Its full advantages are only realized at a very large scale, thereby limiting its accessibility for researchers operating with constrained resources.
\end{itemize}

\section{Conclusion}

This report successfully validated the core contributions of "Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP." Through our independent implementation, we confirm Grad-ECLIP's standing as a cutting-edge technique for producing high-fidelity, reliable, and text-focused explanations tailored for the CLIP model. While our efforts to replicate the fine-tuning application partially confirmed its efficacy, they also underscored its substantial reliance on extensive datasets.

Our analysis positions Grad-ECLIP as an effective extension of Grad-CAM principles to Transformer architectures. We also noted the empirical, rather than strictly theoretical, basis of its "loosened" attention mechanism. This research not only furnishes the community with a potent diagnostic tool but also sheds light on the intricate internal processes of the CLIP model itself.

\subsubsection*{\textbf{Broader Impact Statement}}
Unveiling the rationale behind models such as CLIP brings forth \textbf{considerable upsides}. It significantly \textbf{enhances accountability}, empowering researchers to \textbf{troubleshoot} these systems, \textbf{spot and reduce inherent biases}, and ultimately forge more \textbf{dependable AI frameworks}.

Nevertheless, certain \textbf{detrimental outcomes} could arise. Laypersons might \textbf{misconstrue} these explanations, and a thorough grasp of a model's weaknesses could, regrettably, be \textbf{leveraged by malicious entities}. Consequently, \textbf{prudent application} and \textbf{explicit disclosure of its boundaries} are \textbf{absolutely essential}.

\subsubsection*{Acknowledgments}
We thank the authors of the original paper for making their work public and for providing a clear description of their method, which greatly facilitated this reproducibility study. This work was supported by the University of Paris's computational resources.

\bibliography{tmlr}
\bibliographystyle{tmlr}

\begin{filecontents}{tmlr.bib}
@inproceedings{zhao2024grad,
  title={Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP},
  author={Zhao, Chenyang and Wang, Kun and Hsiao, Janet H and Chan, Antoni B},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  year={2024},
  organization={PMLR}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{gao2022large,
  title={Large-scale unsupervised semantic segmentation},
  author={Gao, Shang-Hua and Li, Zhong-Yu and Yang, Ming-Hsuan and Cheng, Ming-Ming and Han, Jun and Torr, Philip},
  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2556--2565},
  year={2018}
}
\end{filecontents}

\appendix
\section{Appendix}
For a full understanding of the implementation, including the exact scripts for heatmap generation and fine-tuning experiments, consult the code repository. Its documentation also provides additional details on the experimental setup, such as library versions and hyperparameters.

\end{document}